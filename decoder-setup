#!/bin/bash

# Function to log messages with timestamp
log_message() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
}

# Function to check decoder health
check_decoder_health() {
    if pgrep -f 'private_xllm.bin' > /dev/null; then
        if (echo > /dev/tcp/localhost/62000) 2>/dev/null; then
            return 0
        fi
    fi
    return 1
}

log_message "Starting decoder setup..."

# Change to project directory
project_dir=/app/xllm
cd $project_dir

# Set up Python and library paths
export PYTHONPATH=$project_dir:$PYTHONPATH
export LD_LIBRARY_PATH=$project_dir:$LD_LIBRARY_PATH

# GPU configuration
NUM_GPU=`nvidia-smi -L | wc -l`
export XLLM_MODEL_MP_SIZE=$NUM_GPU
export XLLM_PARALLEL_LOCAL_WORLD_SIZE=$NUM_GPU

log_message "Detected $NUM_GPU GPUs"

# Model configuration
export XLLM_MODEL_NAME=deepseek-r1
export XLLM_MODEL_VERSION=250120

# Model directories
DEEPSEEK_MODEL_DIR=/data/models/DeepSeek-R1-ENC-xLLM
DEEPSEEK_DRAFT_MODEL_DIR=/data/models/DeepSeek-R1-DRAFT-ENC-xLLM
export XPERF_TUNER_CONFIG_LOAD_PATH=$DEEPSEEK_MODEL_DIR/saved_model/gemm_config
export XLLM_DRAFT_MODEL_MODEL_DIR=$DEEPSEEK_DRAFT_MODEL_DIR

# Network configuration
LOCAL_ADDR=$(ip -4 addr show eth0 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')
log_message "Local address: $LOCAL_ADDR"

# Splitwise configuration
export SPLITWISE_PREFILL_ROUTING_CONFIG__ROUTING_STRATEGY="p2c"
export SPLITWISE_PREFILL_ROUTING_CONFIG__PICK_N=3
export SPLITWISE_DRIVER_PORT=62001
export SPLITWISE_KV_CACHE_RECEIVER_PORT=62002
export SPLITWISE_PREFILL_LENGTH_THRESHOLD=0
export SPLITWISE_KV_TRANSFER_BACKEND=veturborpc
export SPLITWISE_PREFILL_ROUTING_CONFIG__ROUTING_TABLE_UPDATER_CONFIG__TYPE="env_based"
export SPLITWISE_PREFILL_ROUTING_CONFIG__ROUTING_TABLE_UPDATER_CONFIG__ADDRS_VAR_NAME="PREFILL_ADDRS"

# Prefill configuration - use environment variable if set, otherwise use default
prefill_ip=${PREFILL_NODE_IP:-"<prefill_node_ip>"}
export PREFILL_ADDRS="${prefill_ip}:62001"
export SPLITWISE_DECODE_ADDR=${LOCAL_ADDR}
export SPLITWISE_KV_CACHE_RECEIVER_ADDR=${LOCAL_ADDR}
export VETURBORPC_LOCAL_ADDR=${LOCAL_ADDR}

log_message "Prefill addresses: $PREFILL_ADDRS"

# Processor configuration
export XLLM_PROCESSOR_REASONING_END_TOKEN="</think>"
export XLLM_PROCESSOR_REASONING_PREFILL_TEMPLATE=$'<think>\n'
export XLLM_PROCESSOR_TOOL_PARSER=deepseek_r1

# Environment file path
XLLM_ENV_PATH=/app/xllm/decode_enc.env

log_message "Starting decoder service..."

# Start the decoder service
./private_xllm.bin \
        --model-dir $DEEPSEEK_MODEL_DIR \
        --env-path $XLLM_ENV_PATH \
        --driver-type splitwise_decode \
        --num-server-procs 4 \
        --port 62000 &

DECODER_PID=$!
log_message "Decoder service started with PID: $DECODER_PID"

# Wait for decoder to be healthy
log_message "Waiting for decoder to be healthy..."
HEALTH_CHECK_ATTEMPTS=0
MAX_HEALTH_CHECK_ATTEMPTS=60

while [ $HEALTH_CHECK_ATTEMPTS -lt $MAX_HEALTH_CHECK_ATTEMPTS ]; do
    if check_decoder_health; then
        log_message "Decoder is healthy!"
        break
    fi
    
    HEALTH_CHECK_ATTEMPTS=$((HEALTH_CHECK_ATTEMPTS + 1))
    log_message "Health check attempt $HEALTH_CHECK_ATTEMPTS/$MAX_HEALTH_CHECK_ATTEMPTS - decoder not ready yet..."
    sleep 5
done

if [ $HEALTH_CHECK_ATTEMPTS -eq $MAX_HEALTH_CHECK_ATTEMPTS ]; then
    log_message "ERROR: Decoder failed to become healthy within timeout"
    exit 1
fi

# Signal that decoder is ready
log_message "Decoder setup completed successfully!"

# Step 2: Start API server
log_message "Starting API server..."
cd /app/api-server

# Start the API server in the background

./start.sh &

API_SERVER_PID=$!

log_message "API server started with PID: $API_SERVER_PID"

# Function to check API server health
check_api_health() {
    if curl -s http://localhost/ > /dev/null 2>&1; then
        return 0
    fi
    return 1
}

# Wait for API server to be healthy
log_message "Waiting for API server to be healthy..."
HEALTH_CHECK_ATTEMPTS=0
MAX_HEALTH_CHECK_ATTEMPTS=30

while [ $HEALTH_CHECK_ATTEMPTS -lt $MAX_HEALTH_CHECK_ATTEMPTS ]; do
    if check_api_health; then
        log_message "API server is healthy!"
        break
    fi
    
    HEALTH_CHECK_ATTEMPTS=$((HEALTH_CHECK_ATTEMPTS + 1))
    log_message "Health check attempt $HEALTH_CHECK_ATTEMPTS/$MAX_HEALTH_CHECK_ATTEMPTS - API server not ready yet..."
    sleep 2
done

if [ $HEALTH_CHECK_ATTEMPTS -eq $MAX_HEALTH_CHECK_ATTEMPTS ]; then
    log_message "WARNING: API server failed to become healthy within timeout, but continuing..."
fi

# Step 3: Test the API
log_message "Testing the API..."
sleep 3  # Give API server a moment to fully initialize

log_message "Sending test request to API..."
RESPONSE=$(curl -s -w "\n%{http_code}" http://localhost/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "xllm",
        "messages": [
            {
                "role": "user",
                "content": "Please prove the Riemann Hypothesis."
            }
        ],
        "max_tokens": 100,
        "temperature": 0.7
    }')

HTTP_CODE=$(echo "$RESPONSE" | tail -n1)
RESPONSE_BODY=$(echo "$RESPONSE" | sed '$d')

log_message "HTTP Status Code: $HTTP_CODE"

if [ "$HTTP_CODE" = "200" ]; then
    log_message "SUCCESS: API test completed successfully!"
    log_message "Response:"
    echo "$RESPONSE_BODY" | python3 -m json.tool 2>/dev/null || echo "$RESPONSE_BODY"
else
    log_message "ERROR: API test failed with HTTP code: $HTTP_CODE"
    log_message "Response:"
    echo "$RESPONSE_BODY"
fi

log_message "System is fully operational!"
log_message "Both decoder and API server are running. Container will stay alive."

# Keep both processes running
wait 